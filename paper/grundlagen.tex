\chapter{Grundlagen}

\section{Die Computertomographie}

\subsection{Die Geschichte und prinzipielle Funktionsweise der Computertomographie}

Die Geschichte der Computertomographie beginnt mit dem Röntgenverfahren, welches 1895 vom deutschen Physiker Wilhelm
Conrad Röntgen entdeckt wurde~\cite{roentgen}. Mittels einer Strahlungsquelle wird ein Objekt durchleuchtet und auf
einem Film bzw.\ einem Detektor abgebildet; der dreidimensionale Körper wird also auf eine zweidimensionale Fläche
projiziert. Diesen Schritt bezeichnet man als \textit{Vorwärtsprojektion}.

Führt man die Vorwärtsprojektion genügend oft in aufeinanderfolgenden Winkelschritten aus, bis man (idealerweise) einen
Vollkreis abgefahren hat, so lässt sich aus den dabei entstandenen \textit{Projektionen} der ursprünglich durchleuchtete
Körper, den wir in der Folge als \textit{Volumen} bezeichnen, rekonstruieren. Für jeden Punkt im Volumen
(\textit{Voxel}) kann anhand der Informationen aus den Projektionen der Absorptionsgrad berechnet und dadurch die
innere Struktur des Volumens bestimmt werden. Dieser Zusammenhang wurde in den 60er Jahren des 20. Jahrhunderts durch
den südafrikanisch-amerikanischen Physiker Allan McLeod Cormack festgestellt, der ebenfalls die dazu notwendigen
mathematischen Grundlagen entwickelte~\cite{cormack63}~\cite{cormack64}; ihm war allerdings unbekannt~\cite{cormack79},
dass diese schon 1917 vom österreichischen Mathematiker Johann Radon gefunden wurden~\cite{radon}. Mathematisch ist der
Vorgang der \textit{Rückprojektion} eine Anwendung der nach Radon benannten \textit{Radon-Transformation}.

Ein Problem der Vorwärtsprojektion ist der Informationsverlust, der durch die mangelnde Tiefe des Detektors entsteht;
die Tiefeninformationen werden auf die zweidimensionale Fläche {\glqq}verschmiert{\grqq}. Bei der Rückprojektion lässt
sich dieser Verlust durch die Wahl eines geeigneten Bildfilters wiederum kaschieren, weshalb man auch von der
\textit{gefilterten Rückprojektion} spricht.

Da die gefilterte Rückprojektion für jedes Voxel einzeln berechnet werden muss, ist sie für einen Menschen nicht in
sinnvoller Zeit lösbar. Aus diesem Grund ist man für die Lösung des Gesamtproblems also auf einen Computer angewiesen,
woraus sich der Name des Verfahrens ableitet: \textit{Computertomographie}. Die ersten bis zur Marktreife entwickelten
Computertomographen wurden gegen Ende der 60er Jahre des 20. Jahrhunderts vom englischen Elektroingenieur Godfrey
Hounsfield gebaut. Dieser entwickelte die für die Rückprojektion nötigen Algorithmen ebenfalls selbst, da ihm die
Vorarbeiten von Cormack und Radon nicht bekannt waren~\cite{kalender}. Für ihre voneinander unabhängigen Arbeiten
erhielten Godfrey und Cormack 1979 den Nobelpreis für Physiologie oder Medizin, was die Bedeutung der
Computertomographie insbesondere für die Medizin unterstreicht.

\subsection{Der Feldkamp-Davis-Kress-Algorithmus und seine Parallelisierung}

Der 1984 entwickelte \gls{fdk}~\cite{fdk} ist eine spezielle Ausprägung der gefilterten Rückprojektion. Ausgangspunkt
der Strahlung ist eine Quelle, die das Volumen mit einem \textit{kegelförmigen} Strahl durchleuchtet und damit auf einem
Detektor abbildet. Aufgrund seiner geringen Komplexität und einfachen Implementierbarkeit ist der \gls{fdk} einer der 
beliebtesten Rückprojektionsalgorithmen für die Kegelstrahl-Computertomographie~\cite{xumuell}. Der Vorteil des
\gls{fdk} liegt außerdem darin, dass die gefilterte Rückprojektion für jedes Voxel individuell berechnet werden kann,
das heißt ohne Abhängigkeiten zu anderen Voxeln. Dieser Umstand ermöglicht einen Grad an Parallelität, für den sich im
englischen Sprachraum der Begriff \textit{embarassingly parallel} eingebürgert hat, und macht den \gls{fdk} zu einem
idealen Ziel für diverse Parallelisierungsansätze. Seit seiner Einführung ist der \gls{fdk} daher ein beliebtes
Untersuchungsobjekt von Forschungsgruppen, die sich mit seiner Beschleunigung bzw.\ Parallelisierung mittels einer
großen Variation von Architekturen, Plattformen und Programmiermodellen beschäftigen. Einige neuere Ansätze sollen im
Folgenden vorgestellt werden.

Xu et al. untersuchten bereits 2004, inwieweit sich der \gls{fdk} durch den Einsatz handelsüblicher Grafikkarten
(\textit{commodity graphics hardware}) beschleunigen lässt~\cite{xumuell}. Dabei wurden die Schritte \textit{Wichtung}
und \textit{Filterung} aufgrund ihrer geringen Komplexität ($\mathcal{O}(n^2)$) auf der CPU ausgeführt, während man die
komplexere \textit{Rückprojektion} ($\mathcal{O}(n^4)$) auf der GPU berechnete. Die Rückprojektion fand schichtweise
statt, jeweils für eine Voxelebene entlang der vertikalen Volumenachse. Bereits mit der damals verfügbaren Hardware
wurde mit der verwendeten GPU ein Speedup von 7,5 (32bit Gleitkommazahlen) bzw.\ 37 (8bit Ganzzahlen) gegenüber der
verwendeten CPU erreicht, was in Anbetracht des verfügbaren Grafikkartenspeichers bereits als enorme Beschleunigung zu
werten ist. In ihrem Fazit stellen die Autoren die Vermutung auf, dass der Abstand zwischen den Leistungen von CPU und
GPU in der Zukunft zugunsten der GPUs immer weiter werden würde: \textit{Since GPU performance has so far doubled every
6 months (i.e., triple of Moore's law), we expect that the gap between CPU and GPU approaches will widen even further in
the near future.}

Li et al. beschäftigten sich 2005 damit, wie man den \gls{fdk} mit einem \gls{fpga} implementieren könnte. Dazu teilten sie
das Ausgabevolumen, also die Zieldaten der Rückprojektion, in mehrere Würfel (\textit{bricks}) auf, um zu einer
optimalen Cachenutzung zu kommen. Der von ihnen verwendete deterministische Aufteilungsalgorithmus hatte zur Folge, dass
bei der Berechnung auf dem \gls{fpga} kein Cache-Verfehlen (\textit{cache miss}) mehr auftrat. Auf diese Weise konnten
sie ein 16bit-Festkommazahlen-Volumen mit $1024^3$ Voxeln in 267,7 Sekunden rekonstruieren.

Knaup et al. gingen 2007 der Frage nach, ob der \gls{fdk} durch die Eigenschaften der Cell-Architektur profitieren
könne~\cite{knaupsteck}.

Scherl et al. unternahmen 2008 den Versuch, den \gls{fdk} mittels \gls{cuda} zu beschleunigen~\cite{scherlkeck}.

Balász et al. versuchten 2009 das Gleiche mit der \gls{opencl}~\cite{balgab}.

Hofmann et al. untersuchten eventuelle Vorteile durch den Einsatz der neuen Koprozessoren vom Typ Intel{\textregistered}
Xeon Phi{\texttrademark} 'Knights Corner'~\cite{hoftrei}.

Zhao et al. verfolgten die Absicht, eine Beschleunigung durch Ausnutzung geometrischer Zusammenhänge zu
erreichen~\cite{zhao}.

Welche Vor- und Nachteile haben diese Ansätze? -> Warum brauchen wir etwas neues / warum können wir sie nicht anwenden?

\section{Die NVIDIA{\textregistered}-CUDA{\textregistered}-Plattform}

\subsection{Grafikkarten und Wissenschaft}

\subsection{Das CUDA{\textregistered}-Programmiermodell}

\subsection{Alternativen zu CUDA{\textregistered}}
